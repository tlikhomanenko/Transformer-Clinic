Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.en
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de valid en-de 1061 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 1061 sentences (14016 tokens) in 7.8s (136.08 sentences/s, 1797.59 tokens/s)
| Generate valid with beam=4: BLEU4 = 27.01, 56.5/31.8/20.9/14.1 (BP=1.000, ratio=1.036, syslen=11065, reflen=10683)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.en
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de test en-de 1084 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 1084 sentences (16993 tokens) in 10.2s (106.65 sentences/s, 1671.92 tokens/s)
| Generate test with beam=4: BLEU4 = 26.87, 56.7/32.0/20.7/13.9 (BP=1.000, ratio=1.071, syslen=12993, reflen=12134)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.en
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 valid en-de 944 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 944 sentences (23160 tokens) in 11.7s (80.67 sentences/s, 1979.11 tokens/s)
| Generate valid with beam=4: BLEU4 = 27.89, 59.2/33.4/21.6/14.4 (BP=0.996, ratio=0.996, syslen=18279, reflen=18353)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.en
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 test en-de 934 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 934 sentences (25428 tokens) in 12.9s (72.67 sentences/s, 1978.53 tokens/s)
| Generate test with beam=4: BLEU4 = 26.62, 57.8/32.3/20.3/13.2 (BP=1.000, ratio=1.034, syslen=19396, reflen=18754)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.en
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 valid en-de 995 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 995 sentences (43935 tokens) in 29.0s (34.26 sentences/s, 1512.91 tokens/s)
| Generate valid with beam=4: BLEU4 = 26.46, 59.2/32.8/20.7/13.4 (BP=0.976, ratio=0.977, syslen=34401, reflen=35223)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-18l/checkpoint51.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.en
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 test en-de 985 examples
| loading model(s) from wmt14ende/wmt-admin-18l/checkpoint51.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.2660911083221436
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4176253080368042
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.4557552337646484
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.583907127380371
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.630199670791626
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.7529503107070923
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8113511800765991
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9099045991897583
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.9594428539276123
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.061166763305664
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.1163265705108643
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.2118661403656006
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.2596490383148193
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.3436570167541504
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.4012722969055176
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.484697103500366
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.5396134853363037
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.6135413646698
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.6636037826538086
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.7430202960968018
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.7983763217926025
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.866713762283325
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.919538974761963
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.988020658493042
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.0422868728637695
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.1008799076080322
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.1516671180725098
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.2153100967407227
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.2657406330108643
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.32641863822937
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.3739278316497803
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.4302728176116943
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.4758002758026123
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.5324153900146484
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.5803377628326416
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 0.9387273788452148
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.1223782300949097
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.2924503087997437
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.386089563369751
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5217678546905518
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6330339908599854
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.729089617729187
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8206473588943481
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9292106628417969
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.0200729370117188
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.091033458709717
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.184859275817871
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2792251110076904
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.3621580600738525
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.447354793548584
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.5253138542175293
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5926365852355957
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6762478351593018
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.7510876655578613
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.811336040496826
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8762712478637695
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.946645498275757
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9961705207824707
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.067669630050659
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.13346529006958
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1900153160095215
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.2546355724334717
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.3101017475128174
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.362994432449341
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.4218127727508545
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.4794836044311523
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.5276169776916504
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.5860862731933594
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.6423020362854004
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.682490110397339
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.736142873764038
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.789803981781006
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.8305866718292236
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.8786580562591553
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.931884288787842
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.974813938140869
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.02079963684082
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.071051597595215
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.1121320724487305
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.15955924987793
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.211574077606201
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.25162935256958
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.295857906341553
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.344449520111084
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.382558345794678
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.427379131317139
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.4758429527282715
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.520134925842285
| Translated 985 sentences (44553 tokens) in 32.9s (29.95 sentences/s, 1354.61 tokens/s)
| Generate test with beam=4: BLEU4 = 28.95, 60.1/34.7/22.4/15.1 (BP=1.000, ratio=1.006, syslen=33811, reflen=33618)
