Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.en
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de valid en-de 1061 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 1061 sentences (14006 tokens) in 7.5s (140.70 sentences/s, 1857.34 tokens/s)
| Generate valid with beam=4: BLEU4 = 27.08, 56.8/31.9/20.9/14.2 (BP=1.000, ratio=1.034, syslen=11043, reflen=10683)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.en
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de test en-de 1084 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 1084 sentences (16937 tokens) in 9.2s (118.06 sentences/s, 1844.60 tokens/s)
| Generate test with beam=4: BLEU4 = 26.99, 57.0/32.3/20.8/13.9 (BP=1.000, ratio=1.067, syslen=12951, reflen=12134)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.en
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 valid en-de 944 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 944 sentences (23032 tokens) in 11.3s (83.35 sentences/s, 2033.71 tokens/s)
| Generate valid with beam=4: BLEU4 = 28.20, 59.9/34.1/22.1/14.8 (BP=0.987, ratio=0.987, syslen=18118, reflen=18353)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.en
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 test en-de 934 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 934 sentences (25289 tokens) in 12.7s (73.50 sentences/s, 1989.96 tokens/s)
| Generate test with beam=4: BLEU4 = 26.84, 58.3/32.8/20.5/13.3 (BP=1.000, ratio=1.028, syslen=19284, reflen=18754)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.en
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 valid en-de 995 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 995 sentences (43520 tokens) in 26.7s (37.30 sentences/s, 1631.62 tokens/s)
| Generate valid with beam=4: BLEU4 = 26.48, 59.9/33.2/21.0/13.6 (BP=0.964, ratio=0.965, syslen=33979, reflen=35223)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.en
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 test en-de 985 examples
| loading model(s) from wmt14ende/wmt-admin-cape-18l-gl-10-750k/checkpoint47.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3311342000961304
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4765386581420898
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5288578271865845
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6523181200027466
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7109944820404053
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8284912109375
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8983913660049438
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.999096155166626
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0599963665008545
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1573822498321533
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2222402095794678
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.314647674560547
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.372296094894409
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.4533016681671143
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.5148208141326904
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.5978891849517822
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.6545190811157227
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.7250256538391113
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.7790164947509766
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.852240562438965
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.90907621383667
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.9734578132629395
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 3.0311880111694336
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 3.0969789028167725
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 3.157261848449707
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 3.2145466804504395
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.266835927963257
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.3320114612579346
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.3870689868927
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.443117618560791
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.493117570877075
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.5466787815093994
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.598076343536377
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.651921033859253
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.7022995948791504
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3779929876327515
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.5174992084503174
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6427537202835083
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7283737659454346
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.8379484415054321
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9366902112960815
layer_num: 2, layer_iter: 8.0
decoder en ratio: 2.023421287536621
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.1119391918182373
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.205782890319824
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.288809061050415
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.3611836433410645
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.4406793117523193
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.522794008255005
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.594303846359253
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.669921875
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.739215135574341
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.8022620677948
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.877070426940918
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.9483721256256104
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 3.0090463161468506
layer_num: 7, layer_iter: 22.0
decoder self ratio: 3.0736358165740967
layer_num: 7, layer_iter: 23.0
decoder en ratio: 3.1339316368103027
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 3.184434652328491
layer_num: 8, layer_iter: 25.0
decoder self ratio: 3.2467200756073
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.3104732036590576
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.3652243614196777
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.4249789714813232
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.4821176528930664
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.5358998775482178
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.5919365882873535
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.642204999923706
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.6976253986358643
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.7532715797424316
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.806999921798706
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.850856304168701
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.9031906127929688
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.956148624420166
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 4.001988887786865
layer_num: 13, layer_iter: 40.0
decoder self ratio: 4.04899263381958
layer_num: 13, layer_iter: 41.0
decoder en ratio: 4.105768203735352
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 4.1477952003479
layer_num: 14, layer_iter: 43.0
decoder self ratio: 4.1949968338012695
layer_num: 14, layer_iter: 44.0
decoder en ratio: 4.243821144104004
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.283815383911133
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.330488204956055
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.379575729370117
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.421300411224365
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.466989994049072
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.520658493041992
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.5587334632873535
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.59975004196167
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.651771545410156
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.696578502655029
Use augmentation  True
| Translated 985 sentences (43583 tokens) in 25.9s (38.06 sentences/s, 1684.16 tokens/s)
| Generate test with beam=4: BLEU4 = 29.17, 60.9/35.5/23.1/15.5 (BP=0.984, ratio=0.984, syslen=33074, reflen=33618)
