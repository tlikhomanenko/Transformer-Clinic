Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 9394 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr/valid.en-fr.en
| loaded 9394 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr/valid.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr valid en-fr 9394 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 9394 sentences (158210 tokens) in 37.0s (253.98 sentences/s, 4277.48 tokens/s)
| Generate valid with beam=5: BLEU4 = 45.71, 66.1/49.8/40.1/33.0 (BP=1.000, ratio=1.030, syslen=138994, reflen=134950)
Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 1047 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr/test.en-fr.en
| loaded 1047 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr/test.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr test en-fr 1047 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 1047 sentences (17156 tokens) in 4.4s (238.00 sentences/s, 3899.87 tokens/s)
| Generate test with beam=5: BLEU4 = 39.16, 64.9/44.8/33.0/24.6 (BP=1.000, ratio=1.007, syslen=14495, reflen=14393)
Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 8708 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-2/valid.en-fr.en
| loaded 8708 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-2/valid.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr-2 valid en-fr 8708 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 8708 sentences (286543 tokens) in 73.0s (119.25 sentences/s, 3924.06 tokens/s)
| Generate valid with beam=5: BLEU4 = 44.18, 66.2/48.3/38.3/31.1 (BP=1.000, ratio=1.002, syslen=260503, reflen=259891)
Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 955 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-2/test.en-fr.en
| loaded 955 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-2/test.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr-2 test en-fr 955 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 955 sentences (28782 tokens) in 7.4s (128.86 sentences/s, 3883.47 tokens/s)
| Generate test with beam=5: BLEU4 = 41.65, 67.9/47.7/35.2/26.4 (BP=1.000, ratio=1.006, syslen=25063, reflen=24922)
Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 8752 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-3/valid.en-fr.en
| loaded 8752 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-3/valid.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr-3 valid en-fr 8752 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 8752 sentences (541531 tokens) in 179.0s (48.89 sentences/s, 3024.90 tokens/s)
| Generate valid with beam=5: BLEU4 = 48.77, 71.4/53.6/43.5/36.3 (BP=0.984, ratio=0.984, syslen=497502, reflen=505668)
Namespace(beam=5, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_fr_joined_dict_split_fr-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.8, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 1001 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-3/test.en-fr.en
| loaded 1001 examples from: ../data-bin/wmt14_en_fr_joined_dict_split_fr-3/test.en-fr.fr
| ../data-bin/wmt14_en_fr_joined_dict_split_fr-3 test en-fr 1001 examples
| loading model(s) from wmt14enfr/wmt-admin-cape-6l-5/checkpoint66.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3423633575439453
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4812519550323486
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.5374032258987427
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6644673347473145
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7250423431396484
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8443716764450073
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.9129807949066162
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 2.0132057666778564
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.0738654136657715
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.175696849822998
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2402000427246094
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3804140090942383
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4777315855026245
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.6109930276870728
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.7036185264587402
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7945092916488647
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.9043277502059937
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.9861022233963013
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.058983564376831
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.148163318634033
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.225468873977661
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2852745056152344
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.368985652923584
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4477593898773193
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5083467960357666
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.587754726409912
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.655794620513916
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.710989475250244
Use augmentation  True
| Translated 1001 sentences (49707 tokens) in 14.7s (68.15 sentences/s, 3384.25 tokens/s)
| Generate test with beam=5: BLEU4 = 41.82, 69.5/48.3/35.4/26.4 (BP=0.994, ratio=0.994, syslen=43202, reflen=43478)
