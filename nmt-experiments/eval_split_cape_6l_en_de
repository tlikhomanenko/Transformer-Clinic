Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1063 examples from: ../data-bin/wmt14_en_de_joined_dict_split/valid.en-de.en
| loaded 1063 examples from: ../data-bin/wmt14_en_de_joined_dict_split/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split valid en-de 1063 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 1063 sentences (13814 tokens) in 3.4s (315.17 sentences/s, 4095.72 tokens/s)
| Generate valid with beam=4: BLEU4 = 25.82, 57.7/31.6/20.1/13.3 (BP=0.979, ratio=0.979, syslen=10864, reflen=11100)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1066 examples from: ../data-bin/wmt14_en_de_joined_dict_split/test.en-de.en
| loaded 1066 examples from: ../data-bin/wmt14_en_de_joined_dict_split/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split test en-de 1066 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 1066 sentences (16396 tokens) in 3.7s (290.14 sentences/s, 4462.60 tokens/s)
| Generate test with beam=4: BLEU4 = 26.11, 57.2/31.6/19.9/12.9 (BP=1.000, ratio=1.032, syslen=12560, reflen=12169)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 948 examples from: ../data-bin/wmt14_en_de_joined_dict_split-2/valid.en-de.en
| loaded 948 examples from: ../data-bin/wmt14_en_de_joined_dict_split-2/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split-2 valid en-de 948 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 948 sentences (22891 tokens) in 4.9s (194.97 sentences/s, 4707.96 tokens/s)
| Generate valid with beam=4: BLEU4 = 26.84, 58.7/33.0/21.2/14.0 (BP=0.974, ratio=0.975, syslen=18114, reflen=18582)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 943 examples from: ../data-bin/wmt14_en_de_joined_dict_split-2/test.en-de.en
| loaded 943 examples from: ../data-bin/wmt14_en_de_joined_dict_split-2/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split-2 test en-de 943 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 943 sentences (25227 tokens) in 5.4s (173.91 sentences/s, 4652.45 tokens/s)
| Generate test with beam=4: BLEU4 = 26.99, 58.4/32.7/20.7/13.4 (BP=1.000, ratio=1.012, syslen=19329, reflen=19093)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 989 examples from: ../data-bin/wmt14_en_de_joined_dict_split-3/valid.en-de.en
| loaded 989 examples from: ../data-bin/wmt14_en_de_joined_dict_split-3/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split-3 valid en-de 989 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 989 sentences (42519 tokens) in 10.6s (93.17 sentences/s, 4005.38 tokens/s)
| Generate valid with beam=4: BLEU4 = 25.94, 59.8/33.0/20.4/13.3 (BP=0.960, ratio=0.961, syslen=33219, reflen=34577)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-cape-6l//checkpoint53.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 994 examples from: ../data-bin/wmt14_en_de_joined_dict_split-3/test.en-de.en
| loaded 994 examples from: ../data-bin/wmt14_en_de_joined_dict_split-3/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split-3 test en-de 994 examples
| loading model(s) from wmt14ende/wmt-admin-cape-6l//checkpoint53.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.3260395526885986
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.4717634916305542
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.523785948753357
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.6465873718261719
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.7048959732055664
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.8228371143341064
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.8947384357452393
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.9958305358886719
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 2.058037519454956
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 2.1554994583129883
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 2.2203266620635986
Use augmentation  True
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.3727670907974243
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.4622712135314941
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.5916728973388672
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.6860425472259521
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.7677435874938965
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.8822317123413086
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.963010549545288
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 2.0413501262664795
layer_num: 3, layer_iter: 10.0
decoder self ratio: 2.140543222427368
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.2169687747955322
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.2846312522888184
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.3771021366119385
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.4449141025543213
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.5047836303710938
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.583888292312622
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.6450798511505127
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.7011818885803223
Use augmentation  True
| Translated 994 sentences (43374 tokens) in 10.6s (93.41 sentences/s, 4075.84 tokens/s)
| Generate test with beam=4: BLEU4 = 28.19, 59.7/34.1/21.8/14.6 (BP=0.994, ratio=0.994, syslen=33033, reflen=33244)
