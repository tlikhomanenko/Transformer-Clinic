Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.en
| loaded 1061 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de valid en-de 1061 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 1061 sentences (14004 tokens) in 7.2s (148.00 sentences/s, 1953.44 tokens/s)
| Generate valid with beam=4: BLEU4 = 27.22, 57.1/32.1/21.0/14.3 (BP=1.000, ratio=1.036, syslen=11068, reflen=10683)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.en
| loaded 1084 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de test en-de 1084 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 1084 sentences (16994 tokens) in 9.5s (114.64 sentences/s, 1797.24 tokens/s)
| Generate test with beam=4: BLEU4 = 26.64, 56.8/32.0/20.5/13.6 (BP=1.000, ratio=1.073, syslen=13020, reflen=12134)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.en
| loaded 944 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 valid en-de 944 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 944 sentences (23086 tokens) in 12.8s (73.48 sentences/s, 1797.02 tokens/s)
| Generate valid with beam=4: BLEU4 = 27.77, 59.5/33.6/21.5/14.4 (BP=0.991, ratio=0.991, syslen=18180, reflen=18353)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-2', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.en
| loaded 934 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-2/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-2 test en-de 934 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 934 sentences (25456 tokens) in 13.1s (71.15 sentences/s, 1939.09 tokens/s)
| Generate test with beam=4: BLEU4 = 26.78, 57.8/32.6/20.5/13.3 (BP=1.000, ratio=1.034, syslen=19393, reflen=18754)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='valid', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.en
| loaded 995 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/valid.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 valid en-de 995 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 995 sentences (43982 tokens) in 26.3s (37.85 sentences/s, 1672.95 tokens/s)
| Generate valid with beam=4: BLEU4 = 26.69, 59.6/33.2/20.9/13.7 (BP=0.974, ratio=0.974, syslen=34316, reflen=35223)
Namespace(beam=4, bpe=None, cpu=False, criterion='cross_entropy', data='../data-bin/wmt14_en_de_joined_dict_split_de-3', dataset_impl=None, decoding_format=None, diverse_beam_groups=-1, diverse_beam_strength=0.5, empty_cache_freq=0, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=0.6, load_alignments=False, log_format=None, log_interval=1000, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, num_shards=1, num_workers=1, optimizer='nag', path='wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt', prefix_size=0, print_alignment=False, print_step=False, quiet=True, raw_text=False, remove_bpe='@@ ', replace_unk=None, required_batch_size_multiple=8, results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, unkpen=0, unnormalized=False, upsample_primary=1, user_dir='../radam_fairseq', warmup_updates=0, weight_decay=0.0)
| [en] dictionary: 37184 types
| [de] dictionary: 37184 types
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.en
| loaded 985 examples from: ../data-bin/wmt14_en_de_joined_dict_split_de-3/test.en-de.de
| ../data-bin/wmt14_en_de_joined_dict_split_de-3 test en-de 985 examples
| loading model(s) from wmt14ende/wmt-admin-abspos-18l/checkpoint41.pt
source len: 1024
target len: 1024
1
layer_num: 0, layer_iter: 1.0
encoder attn ratio: 1.0
layer_num: 0, layer_iter: 2.0
encoder ffn ratio: 1.0853325128555298
layer_num: 1, layer_iter: 3.0
encoder attn ratio: 1.2530792951583862
layer_num: 1, layer_iter: 4.0
encoder ffn ratio: 1.2882970571517944
layer_num: 2, layer_iter: 5.0
encoder attn ratio: 1.4342613220214844
layer_num: 2, layer_iter: 6.0
encoder ffn ratio: 1.476204514503479
layer_num: 3, layer_iter: 7.0
encoder attn ratio: 1.6018118858337402
layer_num: 3, layer_iter: 8.0
encoder ffn ratio: 1.644569993019104
layer_num: 4, layer_iter: 9.0
encoder attn ratio: 1.7587207555770874
layer_num: 4, layer_iter: 10.0
encoder ffn ratio: 1.8066749572753906
layer_num: 5, layer_iter: 11.0
encoder attn ratio: 1.9119369983673096
layer_num: 5, layer_iter: 12.0
encoder ffn ratio: 1.9625438451766968
layer_num: 6, layer_iter: 13.0
encoder attn ratio: 2.0615952014923096
layer_num: 6, layer_iter: 14.0
encoder ffn ratio: 2.113302707672119
layer_num: 7, layer_iter: 15.0
encoder attn ratio: 2.204740524291992
layer_num: 7, layer_iter: 16.0
encoder ffn ratio: 2.256350040435791
layer_num: 8, layer_iter: 17.0
encoder attn ratio: 2.3398637771606445
layer_num: 8, layer_iter: 18.0
encoder ffn ratio: 2.3890862464904785
layer_num: 9, layer_iter: 19.0
encoder attn ratio: 2.469719409942627
layer_num: 9, layer_iter: 20.0
encoder ffn ratio: 2.517061233520508
layer_num: 10, layer_iter: 21.0
encoder attn ratio: 2.602234363555908
layer_num: 10, layer_iter: 22.0
encoder ffn ratio: 2.648271322250366
layer_num: 11, layer_iter: 23.0
encoder attn ratio: 2.718982458114624
layer_num: 11, layer_iter: 24.0
encoder ffn ratio: 2.765843629837036
layer_num: 12, layer_iter: 25.0
encoder attn ratio: 2.835488796234131
layer_num: 12, layer_iter: 26.0
encoder ffn ratio: 2.8876872062683105
layer_num: 13, layer_iter: 27.0
encoder attn ratio: 2.958753824234009
layer_num: 13, layer_iter: 28.0
encoder ffn ratio: 3.009899854660034
layer_num: 14, layer_iter: 29.0
encoder attn ratio: 3.0737385749816895
layer_num: 14, layer_iter: 30.0
encoder ffn ratio: 3.1199581623077393
layer_num: 15, layer_iter: 31.0
encoder attn ratio: 3.1832165718078613
layer_num: 15, layer_iter: 32.0
encoder ffn ratio: 3.2276663780212402
layer_num: 16, layer_iter: 33.0
encoder attn ratio: 3.2926692962646484
layer_num: 16, layer_iter: 34.0
encoder ffn ratio: 3.3383471965789795
layer_num: 17, layer_iter: 35.0
encoder attn ratio: 3.399662733078003
layer_num: 17, layer_iter: 36.0
encoder ffn ratio: 3.4491395950317383
layer_num: 0, layer_iter: 1.0
decoder self ratio: 1.0
layer_num: 0, layer_iter: 2.0
decoder en ratio: 1.0941312313079834
layer_num: 0, layer_iter: 3.0
decoder ffn ratio: 1.2405719757080078
layer_num: 1, layer_iter: 4.0
decoder self ratio: 1.3988697528839111
layer_num: 1, layer_iter: 5.0
decoder en ratio: 1.468562364578247
layer_num: 1, layer_iter: 6.0
decoder ffn ratio: 1.5698429346084595
layer_num: 2, layer_iter: 7.0
decoder self ratio: 1.6903572082519531
layer_num: 2, layer_iter: 8.0
decoder en ratio: 1.7646435499191284
layer_num: 2, layer_iter: 9.0
decoder ffn ratio: 1.8512871265411377
layer_num: 3, layer_iter: 10.0
decoder self ratio: 1.9540047645568848
layer_num: 3, layer_iter: 11.0
decoder en ratio: 2.028435707092285
layer_num: 3, layer_iter: 12.0
decoder ffn ratio: 2.0942091941833496
layer_num: 4, layer_iter: 13.0
decoder self ratio: 2.1835291385650635
layer_num: 4, layer_iter: 14.0
decoder en ratio: 2.2541065216064453
layer_num: 4, layer_iter: 15.0
decoder ffn ratio: 2.325608730316162
layer_num: 5, layer_iter: 16.0
decoder self ratio: 2.406017780303955
layer_num: 5, layer_iter: 17.0
decoder en ratio: 2.470841407775879
layer_num: 5, layer_iter: 18.0
decoder ffn ratio: 2.5346555709838867
layer_num: 6, layer_iter: 19.0
decoder self ratio: 2.6109836101531982
layer_num: 6, layer_iter: 20.0
decoder en ratio: 2.678285598754883
layer_num: 6, layer_iter: 21.0
decoder ffn ratio: 2.736043930053711
layer_num: 7, layer_iter: 22.0
decoder self ratio: 2.8096351623535156
layer_num: 7, layer_iter: 23.0
decoder en ratio: 2.876893997192383
layer_num: 7, layer_iter: 24.0
decoder ffn ratio: 2.9255781173706055
layer_num: 8, layer_iter: 25.0
decoder self ratio: 2.998589038848877
layer_num: 8, layer_iter: 26.0
decoder en ratio: 3.0579304695129395
layer_num: 8, layer_iter: 27.0
decoder ffn ratio: 3.1052708625793457
layer_num: 9, layer_iter: 28.0
decoder self ratio: 3.164792776107788
layer_num: 9, layer_iter: 29.0
decoder en ratio: 3.229276657104492
layer_num: 9, layer_iter: 30.0
decoder ffn ratio: 3.277038812637329
layer_num: 10, layer_iter: 31.0
decoder self ratio: 3.337273120880127
layer_num: 10, layer_iter: 32.0
decoder en ratio: 3.3918943405151367
layer_num: 10, layer_iter: 33.0
decoder ffn ratio: 3.436811685562134
layer_num: 11, layer_iter: 34.0
decoder self ratio: 3.494445323944092
layer_num: 11, layer_iter: 35.0
decoder en ratio: 3.5464909076690674
layer_num: 11, layer_iter: 36.0
decoder ffn ratio: 3.5886590480804443
layer_num: 12, layer_iter: 37.0
decoder self ratio: 3.639753818511963
layer_num: 12, layer_iter: 38.0
decoder en ratio: 3.6920523643493652
layer_num: 12, layer_iter: 39.0
decoder ffn ratio: 3.7292873859405518
layer_num: 13, layer_iter: 40.0
decoder self ratio: 3.778701066970825
layer_num: 13, layer_iter: 41.0
decoder en ratio: 3.827716112136841
layer_num: 13, layer_iter: 42.0
decoder ffn ratio: 3.871799945831299
layer_num: 14, layer_iter: 43.0
decoder self ratio: 3.925743818283081
layer_num: 14, layer_iter: 44.0
decoder en ratio: 3.9780466556549072
layer_num: 14, layer_iter: 45.0
decoder ffn ratio: 4.018567085266113
layer_num: 15, layer_iter: 46.0
decoder self ratio: 4.073185443878174
layer_num: 15, layer_iter: 47.0
decoder en ratio: 4.123625755310059
layer_num: 15, layer_iter: 48.0
decoder ffn ratio: 4.161353588104248
layer_num: 16, layer_iter: 49.0
decoder self ratio: 4.20854377746582
layer_num: 16, layer_iter: 50.0
decoder en ratio: 4.2552571296691895
layer_num: 16, layer_iter: 51.0
decoder ffn ratio: 4.285464286804199
layer_num: 17, layer_iter: 52.0
decoder self ratio: 4.328815460205078
layer_num: 17, layer_iter: 53.0
decoder en ratio: 4.371166229248047
layer_num: 17, layer_iter: 54.0
decoder ffn ratio: 4.4044952392578125
| Translated 985 sentences (44080 tokens) in 27.7s (35.50 sentences/s, 1588.83 tokens/s)
| Generate test with beam=4: BLEU4 = 29.49, 60.7/35.3/23.0/15.6 (BP=0.996, ratio=0.996, syslen=33490, reflen=33618)
